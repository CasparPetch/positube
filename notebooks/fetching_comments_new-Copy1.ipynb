{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7ffbcf",
   "metadata": {},
   "source": [
    "# Grabbing comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IDs.csv\")[\"video_id\"][0:10]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.DataFrame(df)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import string\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "download('words')\n",
    "\n",
    "def clean_and_filter(df):\n",
    "    def remove_newline(text):\n",
    "        text = text.replace('\\n', '') \n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        for punctuation in string.punctuation: \n",
    "            text = text.replace(punctuation, '') \n",
    "        return text\n",
    "\n",
    "    def lowercase (text): \n",
    "        lowercased = text.lower() \n",
    "        return lowercased\n",
    "\n",
    "    df['comment_clean'] = df.comment.apply(remove_newline)\n",
    "    df['comment_clean'] = df.comment_clean.apply(remove_punctuation)\n",
    "    df['comment_clean'] = df.comment_clean.apply(lowercase)\n",
    "    \n",
    "    english_words = set(words.words())\n",
    "    def is_english(text):\n",
    "        words_in_comment = word_tokenize(text)\n",
    "        num_words_in_comment = len(words_in_comment)\n",
    "        num_english_words_in_comment = 0\n",
    "        for word in words_in_comment:\n",
    "            if word in english_words:\n",
    "                num_english_words_in_comment += 1\n",
    "        english = False\n",
    "        if num_english_words_in_comment/num_words_in_comment >= 0.3:\n",
    "            english = True\n",
    "        return english\n",
    "    \n",
    "    def english_only(df):\n",
    "        df['english'] = df['comment_clean'].apply(is_english)\n",
    "        return df\n",
    "\n",
    "    df = english_only(df)\n",
    "    \n",
    "    def remove_non_english_symbols(text):\n",
    "        english_pattern = re.compile(r'[^a-zA-Z0-9\\s\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]')\n",
    "        cleaned_text = re.sub(english_pattern, '', text)\n",
    "        return cleaned_text\n",
    "    \n",
    "    \n",
    "    df['comment_clean'] = df.comment_clean.apply(remove_non_english_symbols)\n",
    "    \n",
    "    return df[df[\"english\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = {\n",
    "    \"1\":\"Film & Animation\",\n",
    "    \"2\":\"Autos & Vehicles\",\n",
    "    \"10\":\"Music\",\n",
    "    \"15\":\"Pets & Animals\",\n",
    "    \"17\":\"Sports\",\n",
    "    \"18\":\"Short Movies\",\n",
    "    \"19\":\"Travel & Events\",\n",
    "    \"20\":\"Gaming\",\n",
    "    \"21\":\"Videoblogging\",\n",
    "    \"22\":\"People & Blogs\",\n",
    "    \"23\":\"Comedy\",\n",
    "    \"24\":\"Entertainment\",\n",
    "    \"25\":\"News & Politics\",\n",
    "    \"26\":\"Howto & Style\",\n",
    "    \"27\":\"Education\",\n",
    "    \"28\":\"Science & Technology\",\n",
    "    \"29\":\"Nonprofits & Activism\",\n",
    "    \"30\":\"Movies\",\n",
    "    \"31\":\"Anime/Animation\",\n",
    "    \"32\":\"Action/Adventure\",\n",
    "    \"33\":\"Classics\",\n",
    "    \"34\":\"Comedy\",\n",
    "    \"35\":\"Documentary\",\n",
    "    \"36\":\"Drama\",\n",
    "    \"37\":\"Family\",\n",
    "    \"38\":\"Foreign\",\n",
    "    \"39\":\"Horror\",\n",
    "    \"40\":\"Sci-Fi/Fantasy\",\n",
    "    \"41\":\"Thriller\",\n",
    "    \"42\":\"Shorts\",\n",
    "    \"43\":\"Shows\",\n",
    "    \"44\":\"Trailers\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_key_index = 0\n",
    "current_key_calls = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment\n",
    "# def fetch_comments_relevance(video_id, api_key):\n",
    "#     url = f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&key={api_key}&maxResults=100&order=relevance\"\n",
    "#     comments = []\n",
    "#     nextPageToken = None\n",
    "#     iter_number = 0\n",
    "#     token_count = 0\n",
    "#     while len(comments) < 100:\n",
    "#         print(f\"Tokens used: {token_count}\")\n",
    "        \n",
    "#         if nextPageToken:\n",
    "#             url += f\"&pageToken={nextPageToken}\"\n",
    "        \n",
    "#         response = requests.get(url)\n",
    "#         token_count += 1\n",
    "#         data = response.json()\n",
    "        \n",
    "#         error = data.get(\"error\", False)\n",
    "#         if error:\n",
    "#             return [\"\"]\n",
    "        \n",
    "#         for item in data.get(\"items\", []):\n",
    "#             if item:\n",
    "#                 comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"]\n",
    "#                 author = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n",
    "#                 likecount = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "#                 date = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "#                 replies = item[\"snippet\"][\"totalReplyCount\"]\n",
    "#                 dict_ = {\"comment\": comment, \"author\": author, \"likecount\": likecount, \"date\": date, \"replies\": replies}\n",
    "\n",
    "#                 if comment is not None:\n",
    "#                     cleaned_comment = clean_and_filter(pd.DataFrame([comment],columns=[\"comment\"]))\n",
    "#                     if not cleaned_comment.empty:\n",
    "#                         if cleaned_comment[\"english\"][0] == True:\n",
    "#                             dict_[\"comment_clean\"] = cleaned_comment[\"comment_clean\"]\n",
    "#                             comments.append(dict_)\n",
    "#         nextPageToken = data.get(\"nextPageToken\", None)\n",
    "#         if not nextPageToken or iter_number >= 4:\n",
    "#             break\n",
    "#         if iter_number == 0:\n",
    "#             if len(comments) < 25:\n",
    "#                 break\n",
    "#         iter_number += 1\n",
    "    \n",
    "#     return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "API_KEYS = [\n",
    "    os.getenv('API_KEY'),\n",
    "    os.getenv('API_KEY_1'),\n",
    "    os.getenv('API_KEY_2'),\n",
    "    os.getenv('API_KEY_3'),\n",
    "    os.getenv('API_KEY_4'),\n",
    "    os.getenv('API_KEY_5'),\n",
    "    os.getenv('API_KEY_6'),\n",
    "    os.getenv('API_KEY_7'),\n",
    "    os.getenv('API_KEY_8'),\n",
    "    os.getenv('API_KEY_9'),\n",
    "    os.getenv('API_KEY_10'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d05f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_comments_relevance(video_id, API_KEYS):\n",
    "    max_results = 100\n",
    "    order = \"relevance\"\n",
    "    \n",
    "    comments = []\n",
    "    token_count = 0\n",
    "    idx = 0\n",
    "    \n",
    "    nextPageToken = None\n",
    "    iter_number = 0\n",
    "    url_base = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "    while len(comments) < 100:\n",
    "        print(f\"Tokens used: {token_count}\")\n",
    "        url = f\"{url_base}?part=snippet&videoId={video_id}&key={API_KEYS[idx]}&maxResults=100&order=relevance\"\n",
    "\n",
    "        if nextPageToken:\n",
    "            url += f\"&pageToken={nextPageToken}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        print('RESPONSE: ', response)\n",
    "        print('RESPONSE STATUS: ', response.status_code)\n",
    "        print('OK?: ', response.ok)\n",
    "        if (response.status_code == 403 or response.status_code == 400) and idx < len(API_KEYS):\n",
    "            idx += 1 \n",
    "#             print (f\"I'm using this API key: {API_KEYS[idx]}\")\n",
    "            print (f\"I was at this video id: {video_id}\")\n",
    "            url = f\"{url_base}?part=snippet&videoId={video_id}&key={API_KEYS[idx]}&maxResults=100&order=relevance\"\n",
    "            if nextPageToken:\n",
    "                url += f\"&pageToken={nextPageToken}\"\n",
    "            response = requests.get(url)\n",
    "\n",
    "        if idx >= len(API_KEYS):\n",
    "            print(\"I've run out of API keys\")\n",
    "            break\n",
    "\n",
    "        token_count += 1\n",
    "        data = response.json()\n",
    "\n",
    "        error = data.get(\"error\", False)\n",
    "        if error:\n",
    "            return [\"\"]\n",
    "\n",
    "        for item in data.get(\"items\", []):\n",
    "            if item:\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"]\n",
    "                author = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n",
    "                likecount = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "                date = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "                replies = item[\"snippet\"][\"totalReplyCount\"]\n",
    "                dict_ = {\"comment\": comment, \"author\": author, \"likecount\": likecount, \"date\": date, \"replies\": replies}\n",
    "\n",
    "                if comment is not None:\n",
    "                    cleaned_comment = clean_and_filter(pd.DataFrame([comment],columns=[\"comment\"]))\n",
    "                    if not cleaned_comment.empty:\n",
    "                        if cleaned_comment[\"english\"][0] == True:\n",
    "                            dict_[\"comment_clean\"] = cleaned_comment[\"comment_clean\"]\n",
    "                            comments.append(dict_)\n",
    "\n",
    "        nextPageToken = data.get(\"nextPageToken\", None)\n",
    "        if not nextPageToken or iter_number >= 4:\n",
    "            break\n",
    "        if iter_number == 0:\n",
    "            if len(comments) < 25:\n",
    "                break\n",
    "        iter_number += 1\n",
    "\n",
    "        if token_count >= 10000:\n",
    "            break\n",
    "\n",
    "    if token_count >= 10000:\n",
    "        token_count = 0\n",
    "    \n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stats(VIDEO_IDS, API_KEYS):\n",
    "    results = {}\n",
    "    idx = 0\n",
    "    \n",
    "    for v_i, video_id in enumerate(VIDEO_IDS):\n",
    "        url = f\"https://www.googleapis.com/youtube/v3/videos?part=statistics&id={video_id}&key={API_KEYS[idx]}\"\n",
    "        response = requests.get(url)\n",
    "        print('RESPONSE: ', response)\n",
    "        print('RESPONSE STATUS: ', response.status_code)\n",
    "        print('OK?: ', response.ok)\n",
    "\n",
    "        if (response.status_code == 403 or response.status_code == 400) :\n",
    "#             print (f\"I was using this API key: {API_KEYS[idx]}\")\n",
    "            print (f\"I was at this video id: {video_id}\")\n",
    "            results.setdefault(API_KEYS[idx], {})\n",
    "            results[API_KEYS[idx]][video_id] =  {'FINAL VIDEO'}\n",
    "            if idx + 1 < len(API_KEYS):\n",
    "                idx += 1\n",
    "                api_key = API_KEYS[idx]\n",
    "            else:\n",
    "                print('I am now out of API KEYS - these are the remaining videos:')\n",
    "                print(VIDEO_IDS[v_i:])\n",
    "                return results, VIDEO_IDS[v_i:]\n",
    "        \n",
    "\n",
    "        else:\n",
    "            data = response.json()\n",
    "            error = data.get(\"error\",False)\n",
    "            if not error:\n",
    "                data = data.get(\"items\",False)\n",
    "                if data:\n",
    "                    views = data[0][\"statistics\"][\"viewCount\"]\n",
    "                    likes = data[0][\"statistics\"][\"likeCount\"]\n",
    "                    comments = data[0][\"statistics\"][\"commentCount\"]\n",
    "                    dict_ = {\"views\":[views], \"likes\":[likes], \"comments\":[comments]}\n",
    "                    results.setdefault(API_KEYS[idx], {})\n",
    "                    results[API_KEYS[idx]][video_id] =  dict_\n",
    "                    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_details(video_id, API_KEYS):\n",
    "    idx = 0\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/videos?part=snippet&id={video_id}&key={API_KEYS[idx]}\"\n",
    "    url_base = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    print('RESPONSE: ', response)\n",
    "    print('RESPONSE STATUS: ', response.status_code)\n",
    "    print('OK?: ', response.ok)\n",
    "    \n",
    "    if (response.status_code == 403 or response.status_code == 400) and idx < len(API_KEYS):\n",
    "        idx += 1 \n",
    "#         print (f\"I'm using this API key: {API_KEYS[idx]}\")\n",
    "        print (f\"I was at this video id: {video_id}\")\n",
    "        url = f\"{url_base}?part=snippet&videoId={video_id}&key={API_KEYS[idx]}&maxResults=100&order=relevance\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "    data = response.json()\n",
    "    error = data.get(\"error\",False)\n",
    "    if not error:\n",
    "        data = data.get(\"items\",False)\n",
    "        if data:\n",
    "            date = data[0][\"snippet\"][\"publishedAt\"]\n",
    "            channel_id = data[0][\"snippet\"][\"channelId\"]\n",
    "            title = data[0][\"snippet\"][\"title\"]\n",
    "            description = data[0][\"snippet\"][\"description\"]\n",
    "            channel_title = data[0][\"snippet\"][\"channelTitle\"]\n",
    "            tags = data[0][\"snippet\"].get(\"tags\",[\"\"])\n",
    "            genre = genre_dict[data[0][\"snippet\"][\"categoryId\"]]\n",
    "            language = data[0][\"snippet\"].get(\"defaultAudioLanguage\",\"\")\n",
    "            dict_ = {\"date\":date, \"channel_id\":channel_id, \"title\":title, \"description\":description, \"channel_title\":channel_title, \"tags\":[tags], \"genre\":genre, \"language\":language}\n",
    "            return dict_\n",
    "        return {}\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65465fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_info(video_id, api_key):\n",
    "    details = pd.DataFrame(fetch_details(video_id, api_key))\n",
    "    print(details)\n",
    "    print(details[\"language\"][0])\n",
    "    if details[\"language\"][0] in (\"en\", \"en-GB\", \"en-US\"):\n",
    "        comments_relevance = pd.DataFrame(fetch_comments_relevance(video_id, api_key))\n",
    "        stats = pd.DataFrame(fetch_stats(video_id, api_key))\n",
    "        info_all = pd.concat([stats,details],axis=1)\n",
    "        return comments_relevance, info_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_IDS = [\"--0bCF-iK2E\", \"uL6AZ4fiJeo\", 'uzlZm_ROi4c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = fetch_stats(VIDEO_IDS, API_KEYS)\n",
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9624274",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ffce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infos = []\n",
    "comments = []\n",
    "for id in comments_df[\"video_id\"]:\n",
    "    print(id)\n",
    "    result = fetch_all_info(id, API_KEY)\n",
    "    if result:\n",
    "        comment, info = result\n",
    "        infos.append(info)\n",
    "        comments.append(comment)\n",
    "infos = pd.concat(infos)\n",
    "comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7620c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add video Id to infos dataframe ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172a372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_newline(text):\n",
    "    text = text.replace('\\n', '') \n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation: \n",
    "        text = text.replace(punctuation, '') \n",
    "    return text\n",
    "\n",
    "def lowercase (text): \n",
    "    lowercased = text.lower() \n",
    "    return lowercased\n",
    "\n",
    "for df in comments:\n",
    "    df['comment_clean'] = df.comment.apply(remove_newline)\n",
    "    df['comment_clean'] = df.comment_clean.apply(remove_punctuation)\n",
    "    df['comment_clean'] = df.comment_clean.apply(lowercase)\n",
    "comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7854f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "download('words')\n",
    "\n",
    "english_words = set(words.words())\n",
    "def is_english(text):\n",
    "    words_in_comment = word_tokenize(text)\n",
    "    num_words_in_comment = len(words_in_comment)\n",
    "    num_english_words_in_comment = 0\n",
    "    for word in words_in_comment:\n",
    "        if word in english_words:\n",
    "            num_english_words_in_comment += 1\n",
    "    english = False\n",
    "    if num_english_words_in_comment/num_words_in_comment >= 0.3:\n",
    "        english = True\n",
    "    return english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_english(\"ÊÄñ„Åè„Å¶„ÇÇah ah Êï£„Çã„ÅÆ„ÅïËèØÈ∫ó„Å´Áæé„Åó„Åèüé∂ Â§ßÂ•Ω„Åç„Åß„Åôüòç\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_only(df):\n",
    "    df['english'] = df['comment_clean'].apply(is_english)\n",
    "    return df\n",
    "    \n",
    "for df in comments:\n",
    "    df = english_only(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34f2ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comments[3][comments[3][\"english\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english_symbols(text):\n",
    "    english_pattern = re.compile(r'[^a-zA-Z0-9\\s\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]')\n",
    "    cleaned_text = re.sub(english_pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68503800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for df in comments:\n",
    "    df['comment_clean'] = df.comment_clean.apply(remove_non_english_symbols)\n",
    "comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73230b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[3][comments[3][\"english\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f021caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_non_english_symbols(\"i never worry about life life„ÅÆ„Å®„Åì„Çç„ÅÆËìÆÂêõ„ÅÆÂ£∞„Åå„Åô„Åî„ÅèÂ•Ω„Åç„Åß„ÅôÔºÅ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = comments[0].drop(\"comment_clean\",axis=1).drop(\"english\",axis=1)\n",
    "example_clean_filtered = clean_and_filter(example)\n",
    "example_clean_filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
