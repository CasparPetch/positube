{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e231fb58",
   "metadata": {},
   "source": [
    "## ## (0) The NTLK library (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321867cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d2b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When importing nltk for the first time, we need to also download a few built-in libraries\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9706b5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    --0bCF-iK2E\n",
       "1    --14w5SOEUs\n",
       "2    --40TEbZ9Is\n",
       "3    --4tfbSyYDE\n",
       "4    --DKkzWVh-E\n",
       "Name: video_id, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IDs.csv\")[\"video_id\"][0:5]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739801e4",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8023d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    --0bCF-iK2E\n",
       "1    --14w5SOEUs\n",
       "2    --40TEbZ9Is\n",
       "3    --4tfbSyYDE\n",
       "4    --DKkzWVh-E\n",
       "Name: video_id, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39bb2c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_video_comments(video_id, api_key):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&key={api_key}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    error = data.get(\"error\",False)\n",
    "    if not error:\n",
    "        comments = []\n",
    "        for item in data.get(\"items\",False):\n",
    "            if item:\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"]\n",
    "                author = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n",
    "                likecount = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "#                 replies = item[\"snippet\"][\"topLevelComment\"][\"totalReplyCount\"]\n",
    "                comments.append(comment)\n",
    "        return comments\n",
    "    else:\n",
    "        return [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ddbda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "API_KEY = os.environ.get('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff414f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>--0bCF-iK2E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>--14w5SOEUs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     video_id\n",
       "0           0  --0bCF-iK2E\n",
       "1           1  --14w5SOEUs"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IDs.csv\")[0:2]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02c2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd818a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--0bCF-iK2E\n",
      "--14w5SOEUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Trash',\n",
       "  'Was he faster back then😂',\n",
       "  'Its June 2023, just here to confirm that Manchester United finishes players',\n",
       "  'Poochester united has ruined another talent',\n",
       "  'His whole play style is built on confidence, he needs to work on mindset',\n",
       "  'Where is this Sancho when you need him at man United this guy was legit',\n",
       "  'Where this Sancho?',\n",
       "  'Knp sekarang dia g pede untuk melakukan skil² individu nya yah??',\n",
       "  'Almost forgot how well he can play',\n",
       "  'Do u want him back 😂😂😂',\n",
       "  'After seeing this I knew bundesliga is easy and premiere league is more harder.',\n",
       "  'But what happened to him at United?',\n",
       "  \"I'm seeing basic dribbles here nothing outstanding. It's the ice cold finishing which impresses me. He's so cool in front of goal like he has all the time in the world\",\n",
       "  'So sad EPL fans are  yet to experince his Sancho',\n",
       "  \"I remember this Guy... It's January 2023 and I pray he can get back to his best.. That controlled pace is very rare but needs serious confidence which he's lost (at the time of writing)\",\n",
       "  'If anybody could ressurect him ,it would be Ten Hag.',\n",
       "  'BRO What happen to Sancho!! 😭GOD PLEASE I NEED THIS SANCHO BACK AT MANUTD',\n",
       "  'I love you sancho',\n",
       "  'I waiting Sancho come back for Manchester  United.',\n",
       "  'dortmund should try to sign him again .'],\n",
       " ['RIP to @Takeoff,',\n",
       "  '😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔',\n",
       "  'yes 😣😔😔😔😔😔😔😔😔',\n",
       "  'RIP takeoff 🕊️🕊️',\n",
       "  'The Orchestra Though ‼️💯🎯✔️',\n",
       "  '🔥🔥💯 QUAVO 🐐',\n",
       "  'Why u after the bag im after control of the mob moneys the distraction. Im just chilling relaxing',\n",
       "  'Call me bloody Harry 😅 i dont sell dope i just tell tall tales',\n",
       "  'Black and white the suit black and white the benz 😅',\n",
       "  'I love this song on so many levels especially as the sole provider for my ☀️ and try my hardest to provide for all his needs emotionally physically spiritually financially anywho I was having a mommy day 😭🤯😭 & takeoff was like  put this on & I seen my son rapping this 😂 brought a joy like when I first seen him born',\n",
       "  'It\\'s gone be hard not hearing Takeoff on the track with his \"HEY\" \"MOMMA\"',\n",
       "  \"This album was very hot # so I don't know why my friend OFFSET and QUAVO you don't want to forgive each other and you return back the MIGOS GANG because we miss your music and after all QUAVO and OFFSET you're family so you need to forgive each other as brothers 🙏🙏🙏\",\n",
       "  'These guys had musical harmony. Distinct and individual flows, supporting each other with perfect adlibs, each of them riding the beat their own way. They did something memorable.',\n",
       "  'New culture unlocked 🔓🔥',\n",
       "  'chill',\n",
       "  'Offset 💎',\n",
       "  'MOMMA 😣😣🕊️',\n",
       "  '0:40 mommaaa',\n",
       "  'LLtake 🕊️🕊️🕊️❤️😞😞',\n",
       "  'LLtake 🕊️🕊️🕊️❤️😞😞']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos = []\n",
    "for id in df[\"video_id\"]:\n",
    "    print(id)\n",
    "    comments = fetch_video_comments(id, API_KEY)\n",
    "    videos.append(comments)\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4266c523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trash',\n",
       " 'Was he faster back then😂',\n",
       " 'Its June 2023, just here to confirm that Manchester United finishes players',\n",
       " 'Poochester united has ruined another talent',\n",
       " 'His whole play style is built on confidence, he needs to work on mindset',\n",
       " 'Where is this Sancho when you need him at man United this guy was legit',\n",
       " 'Where this Sancho?',\n",
       " 'Knp sekarang dia g pede untuk melakukan skil² individu nya yah??',\n",
       " 'Almost forgot how well he can play',\n",
       " 'Do u want him back 😂😂😂',\n",
       " 'After seeing this I knew bundesliga is easy and premiere league is more harder.',\n",
       " 'But what happened to him at United?',\n",
       " \"I'm seeing basic dribbles here nothing outstanding. It's the ice cold finishing which impresses me. He's so cool in front of goal like he has all the time in the world\",\n",
       " 'So sad EPL fans are  yet to experince his Sancho',\n",
       " \"I remember this Guy... It's January 2023 and I pray he can get back to his best.. That controlled pace is very rare but needs serious confidence which he's lost (at the time of writing)\",\n",
       " 'If anybody could ressurect him ,it would be Ten Hag.',\n",
       " 'BRO What happen to Sancho!! 😭GOD PLEASE I NEED THIS SANCHO BACK AT MANUTD',\n",
       " 'I love you sancho',\n",
       " 'I waiting Sancho come back for Manchester  United.',\n",
       " 'dortmund should try to sign him again .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108c5120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trash\n",
      "was he faster back then😂\n",
      "its june 2023, just here to confirm that manchester united finishes players\n",
      "poochester united has ruined another talent\n",
      "his whole play style is built on confidence, he needs to work on mindset\n",
      "where is this sancho when you need him at man united this guy was legit\n",
      "where this sancho?\n",
      "knp sekarang dia g pede untuk melakukan skil² individu nya yah??\n",
      "almost forgot how well he can play\n",
      "do u want him back 😂😂😂\n",
      "after seeing this i knew bundesliga is easy and premiere league is more harder.\n",
      "but what happened to him at united?\n",
      "i'm seeing basic dribbles here nothing outstanding. it's the ice cold finishing which impresses me. he's so cool in front of goal like he has all the time in the world\n",
      "so sad epl fans are  yet to experince his sancho\n",
      "i remember this guy... it's january 2023 and i pray he can get back to his best.. that controlled pace is very rare but needs serious confidence which he's lost (at the time of writing)\n",
      "if anybody could ressurect him ,it would be ten hag.\n",
      "bro what happen to sancho!! 😭god please i need this sancho back at manutd\n",
      "i love you sancho\n",
      "i waiting sancho come back for manchester  united.\n",
      "dortmund should try to sign him again .\n"
     ]
    }
   ],
   "source": [
    "for i in videos[0]:\n",
    "    print(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "837895f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) Cleaning the (text) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024e0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (1.1) Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812d58e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/jsfzlbbn33g7fs7xcq1cmdf80000gn/T/ipykernel_2818/714932617.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.6/envs/positube/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation: \n",
    "        text = text.replace(punctuation, ' ') \n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df.text.apply(remove_punctuation)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4f20a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (1.2) Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214e1f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'clean_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/jsfzlbbn33g7fs7xcq1cmdf80000gn/T/ipykernel_2818/2328179198.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlowercased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlowercased\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.6/envs/positube/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'clean_text'"
     ]
    }
   ],
   "source": [
    "def lowercase (text): \n",
    "    lowercased = text.lower() \n",
    "    return lowercased\n",
    "\n",
    "df['clean_text'] = df.clean_text.apply(lowercase)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eb70b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (1.3) Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5411fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'clean_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/jsfzlbbn33g7fs7xcq1cmdf80000gn/T/ipykernel_2818/2081134326.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_numbers\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwords_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.6/envs/positube/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'clean_text'"
     ]
    }
   ],
   "source": [
    "def remove_numbers (text):\n",
    "    words_only = ''.join([i for i in text if not i.isdigit()])\n",
    "    return words_only\n",
    "\n",
    "df['clean_text'] = df.clean_text.apply(remove_numbers)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce6ef7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (1.4) Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33a2a873",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'clean_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9k/jsfzlbbn33g7fs7xcq1cmdf80000gn/T/ipykernel_2818/2526411490.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwithout_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwithout_stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.6/envs/positube/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'clean_text'"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Create function\n",
    "def remove_stopwords (text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    without_stopwords = [word for word in tokenized if not word in stop_words]\n",
    "    return without_stopwords\n",
    "\n",
    "df['clean_text'] = df.clean_text.apply(remove_stopwords)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "370ebbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (1.5) Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1fe4a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3520692296.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    0        I love\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Example dataframe\n",
    "df = pd.DataFrame({'text': ['I love 🍕', 'This is a 😊 test', '🌞 Good morning!']})\n",
    "\n",
    "# Function to remove emojis using NLTK\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"u\"\\U0001F300-\\U0001F5FF\"u\"\\U0001F680-\\U0001F6FF\"u\"\\U0001F1E0-\\U0001F1FF\"u\"\\U00002702-\\U000027B0\"u\"\\U000024C2-\\U0001F251\"\"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the function to remove emojis from the 'text' column\n",
    "df['text'] = df['text'].apply(remove_emojis)\n",
    "\n",
    "print(df)\n",
    "\n",
    "0        I love \n",
    "1  This is a  test\n",
    "2   Good morning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8c5d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (1.6) Check and filter for language (delete everything is not in English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf95ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m english_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(words\u001b[38;5;241m.\u001b[39mwords())\n\u001b[1;32m      2\u001b[0m filtered_dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset\u001b[49m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Tokenize the sentence into words\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     words_in_sentence \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Filter out non-English words\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "english_words = set(words.words())\n",
    "filtered_dataset = []\n",
    "\n",
    "for sentence in df:\n",
    "    # Tokenize the sentence into words\n",
    "    words_in_sentence = word_tokenize(sentence)\n",
    "    \n",
    "    # Filter out non-English words\n",
    "    english_words_in_sentence = [word for word in words_in_sentence if word.lower() in english_words]\n",
    "    \n",
    "    # Reconstruct the filtered sentence\n",
    "    filtered_sentence = ' '.join(english_words_in_sentence)\n",
    "    \n",
    "    # Add the filtered sentence to the filtered dataset\n",
    "    filtered_dataset.append(filtered_sentence)\n",
    "    \n",
    "# In this example, dataset represents your original dataset, and filtered_dataset will store the filtered sentences containing only English words.\n",
    "\n",
    "# Remove stopwords (optional): If you want to further refine your dataset, you can also remove common English stopwords (e.g., \"the,\" \"is,\" \"and,\" etc.). NLTK provides a list of stopwords that you can use. Here's an example of how to remove stopwords:\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "filtered_dataset = []\n",
    "\n",
    "for sentence in dataset:\n",
    "    # Tokenize the sentence into words\n",
    "    words_in_sentence = word_tokenize(sentence)\n",
    "    \n",
    "    # Filter out non-English words and stopwords\n",
    "    english_words_in_sentence = [word for word in words_in_sentence if word.lower() in english_words and word.lower() not in english_stopwords]\n",
    "    \n",
    "    # Reconstruct the filtered sentence\n",
    "    filtered_sentence = ' '.join(english_words_in_sentence)\n",
    "    \n",
    "    # Add the filtered sentence to the filtered dataset\n",
    "    filtered_dataset.append(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (1.7) Convert emoji into sentiment somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0e9b0e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3865760079.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install emoji\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install emoji\n",
    "\n",
    "import emoji\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5710a079",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentimentIntensityAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sia \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentimentIntensityAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3930d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji_sentiment(emoji_str):\n",
    "    emoji_text = emoji.demojize(emoji_str)\n",
    "    sentiment_scores = sia.polarity_scores(emoji_text)\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79343c65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emoji' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m emoji_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m😄\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m sentiment_scores \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_emoji_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43memoji_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentiment_scores)\n",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m, in \u001b[0;36mconvert_emoji_sentiment\u001b[0;34m(emoji_str)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_emoji_sentiment\u001b[39m(emoji_str):\n\u001b[0;32m----> 2\u001b[0m     emoji_text \u001b[38;5;241m=\u001b[39m \u001b[43memoji\u001b[49m\u001b[38;5;241m.\u001b[39mdemojize(emoji_str)\n\u001b[1;32m      3\u001b[0m     sentiment_scores \u001b[38;5;241m=\u001b[39m sia\u001b[38;5;241m.\u001b[39mpolarity_scores(emoji_text)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentiment_scores\n",
      "\u001b[0;31mNameError\u001b[0m: name 'emoji' is not defined"
     ]
    }
   ],
   "source": [
    "emoji_str = \"😄\"\n",
    "sentiment_scores = convert_emoji_sentiment(emoji_str)\n",
    "print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34c838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6613bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentence):\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "\n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a11ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = documents[\"documents\"].apply(cleaning)\n",
    "cleaned_documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717b8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
